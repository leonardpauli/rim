# transformation
' rimpy
	created by Leonard Pauli, 14 apr 2019 

transformation:
	// TODO: unify Directory, File under one type Module (directory exports its children?)
	Repository as Directory -> File -> StringLines
	-> Line
	-(resolve_children)> Block
		- line.top
			-(tokenize)> Token
			-(astify)> Lexeme
			-(nodify)> Node
			-> child_ctx
		-	child_ctx + line.children -(recurse)> Block
	-> Line as Block w Lexeme(w Token)
	-> Node
	-> SyntaxPackage
		- build Node, syntax issues, linting issues // logic issues?
	- identifier table construction
	- identifier resolution, identifier issues
	->
		- -> VisualProxy(Text)
		- -> VisualProxy(GUI)
			-> presentation.target specific: ViewModel -> View
			' interaction in View
				-> ViewModel -> action
				-> VisualProxy -> action
				-> Node (modifies node)
					- client
						-> VisualProxy -> ViewModel reacts to change, updates View
					- server
						-> VisualProxy -> ViewModel (AST) reacts to change, updates View (text file)
							+ itself (eg. sets linenr to dirty to trigger lazy recalculation when needed)


repository = Directory "some/path"
ctx = Node.Context
root = repository as Node with ctx
do root.load with ctx
do log ctx as Normalized as Json


' Todo: outline: (OUTDATED, see groupify)
	- identifier register step 1, Lexeme type id only
	- tokenizing
	- parsing to lexemes
	- identifier resolve step 1.1, Lexeme type paren w id (sibling)
	- identifier resolve step 1.2, Lexeme type paren w id (upstream)
	- identifier resolve step 1.3, Lexeme type paren w id.strip (sibling, downstream)
		- upstream downstream order resolve / resolve unresolved afterwards?
	- listitem syntax package (-)
		- identifier register list index?
		- how to save lists?
	- keyvalue syntax package
		- grouping, left? a: b: c -> a: (b: (c)) -> ((:) a ((:) b c))
			- ohh, see lpasm notes about how to do this more generally!
		- identifier register key?
		- handle linting
		- handle lint.autofix flag; add to ctx possible lint fixes list
			- when apply is invoked, update token + set position to dirty
			- in separate step? Node.File -> lines -> indented + raw text from lexems w tokens -> write back
	- simpler interface.dot from normalized connections + nodes (+ identifier resolve)
	- start python coding
	- simpler interface.timeflow.dot based on prev but added dependency list
		// - dot draw w fixed position
	- simpler interface.terminal.full
		- add fixed time
		- config available timeslots
		- assume 1hr def/task,
		- layout/suggest time (1 char / timeslot, timeline to right, indented list to left), print to file
		- shadoww
	- start python coding
	- start planning
	// - comma list building operator
	// - semicolon statement list building operator (eg in paren, braces)
	// - strings, for now throw error for interpolation + skip handle escape + unicode

' 
	identifier_table_construction:
			// register_self: ...
			or self.lexeme:
				- .count is 1 and is Lexeme.Id:
					...
				// - .count is 1 and is Lexeme.Id.Strip: ...
				// ...
				- else: ... // TODO: raise unhanded identifier lexeme case
		identifier_resolution_partial: // (upstream), keep track of unresolved
			// TODO
	' TODO:
			- partial identifier resolution (child downstream/subtree + child siblings), keep track of unresolved
	' TODO: SyntaxPackage?
		keyvalue?



Repository
Directory is Repository
	path is String // url?
	list is many (Directory or File)
	as Node with ctx as Node.Context:
		do ctx.directory[self].node |= Node.Loadable.Directory
			directory: self
File
	// parent is Directory
	name is String
	line is many String // text lines without line-endings
	as Node with ctx as Node.Context:
		do ctx.file[self].node |= Node.Loadable.File
			file: self
Line
	file is File
	nr is Int
	indent
		depth is Int
		raw is String
		// count (\t|  ), if ^[ \t] after that, trigger Issue.Syntax.Indent
		// if ' ' in raw, trigger Issue.Lint.Indent
	text
		raw is String // inc. indent, useful when parsing string block and raw indent wanted
		value is String // part after indent
	child is many Line
	token is many Token // eg. [Id, ".", String.Start, String.Raw, String.End, ".", Id, ":", Space, "(", Id, Id, ")"]
	lexeme is many Lexeme
	' lexeme:
		' output from aster, named lexeme instead of ast node because:
			using the aster more like a tokeniser of tokens;
			the resulting structure is mostly flat (vs. tree like, though still a tree ...),
				list of items consiting of tokens that together form some sort of meaning
					that could be described with a different set of tokens, ie. lexemes
			eg. Id.Strip could be replaced with a direct Id
			eg. keyvalue is a construct in a later pass
			parens and strings form inline blocks, these are also taken during this "astify" process
		- Id.Strip
			part
				- id
					token: Id
				- string
					part
						- raw
							token: String.Raw
					token.start: String.Start // to know AST node bounds, eg. for when replacing
					token.end String.End
				- id
					token: Id
			token.start: Id
			token.end: Id
		- Id.Special
			token: ":"
		- Space
			token: space
		- Group.Paren
			- Id
				token: id
			- Id
				token: id
	' many lexeme as Node:
		// Id.Strip, Id.Special(":"), Space, Group.Paren, Id.Special(","), Id, Group.Paren, Group.Brace
		
			- Id.Special(":") Id.Strip, (Space, Group.Paren, Id.Special(","), Id, Group.Paren, Group.Brace)
			- Id.Special(":") Id.Strip, (Space, Group.Paren, Id.Special(","), Id, Group.Paren, Group.Brace)
			- SyntaxPackage
				- KeyValue.colon

	many from file as File: file.line |> Self{nr: .index + 1, file, raw: it}
	as Node with ctx is {parse is ParseContext, main is Node.Context}:

		self.token: do or Self.tokenize{ctx: ctx.parse} self:
			- Empty or {rest_string not Empty}:
				' TODO
					ctx.issue.syntax.add Error ...
					mark node as failed, return
			- else: .token

		self.lexeme: do or Self.astify{ctx: ctx.parse} token:
			- Empty or {rest_token not Empty}:
				' TODO
					ctx.issue.syntax.add Error ...
					mark node as failed, return
			- else: .val

		be node: self.lexeme as Node.Line{line: self} with {ctx: ctx.main}
		
		ctx.main.node.add node // register + to prevent cyclic infinite (though not for file lines, just for files/(dirs/repos)?)
		node.subnode: line.child |> as Node with {ctx.parse: node.line_child_context, ctx.main} // walk the tree

		


	ParseContext
		flag:
			comment.top.warn: "Top level comments (#) are reserved for depth 0, ie. no indentation, to mark larger sections of a file"
		FileRoot
			flag:
				comment.top.warn: false


Token
	type is Type
		regex is Regex
		match (String, start_index) is Empty or (token, start_index_new)

		Comment.Top.Hashbang "^#!(.*)$"
		Comment.Top "^#( ?)([^/]+|/[^/])*"
		Comment.Block "^'( ?)([^/]+|/[^/])*"
		Id: "^[^ \t.,:;\"'()\[\]{}@#0-9-][^ \t.,:;\"'()\[\]{}@#-]*" // everything that isn't something else, though only guaranteed for [A-Za-z](_?[A-Za-z0-9])*, eg. '$' is not guaranteed to be treated as part of identifier
		Space: "^[ \t]+"
		String.Start: "^\""
			push: String.Ctx
				tokens:
					String.End: "^\""
						pop: 1
					String.End.Unfinished: "^$"
						pop: 1
					String.Escape.Start: "^\\"
						push: String.Escape.Ctx
							tokens:
								String.Escape.Char: "^."
									pop: 1
								String.Escape.Line: "^$"
									pop: 1
					String.Raw: "^[^\\]"
		...
		Literal: ".,:;\"'()[]{}@#-"
	index
		start is Int
		end is Int
		length is Int
			value:
				if line is token.dirty and line.token.dirty.start <= self.start:
					do recalculate index for tokens(line.token.dirty..self)
					if self is last, do line.token.dirty = false
				be end - start
			on update:
				do end = start + length
				if self isn't last, do line.token.dirty = self
	group is many (group_name_or_index: String)

AST

Lexeme
	line_child_context is Line.ParseContext:
		' TODO:
			// {parent_ctx is Line.ParseContext} is provided to ParseContext fns when used
			find lexeme corresponding to last line token,
				if finished, return parent context,
				else, if open, (eg. string not closed, return string parse context with parent context)
	--
	Space
	Id
		Strip
		Special
	String
	Comment
		Top
		Line
		Block
	Group
		Paren // ()
		Brace // {}
		Bracket // []
	--
	many as Node with {ctx is Node.Context}:
		ctx.parse: self.line_child_context
		be node: groupify it

		... go through node and add identifier (resolved + unresoved) to ctx?
			+ find child_line_ctx
			+ groupify should attach lexeme info
			+ new Line.Computed:
				+ line.lexeme should be computed from node
				+ line.token should be computed from line.lexeme
				+ line.raw should be computed from line.token

	groupify many self: | or as (a b ...c): // see lpasm
		- a is Group:
			- b is Space: Bind(a, ...groupify c)
			- else: groupify Group(a){attachment: b} ...c
		// - a b ...c -> a b ...(...c) if a is group
		- a b ...c -> b a ...(...c)
		SyntaxPackage?


Node
	subnode is many Node
	cache ...
	// identifier is many (name: Identifier)
	--
	Line is Self
		line is Line
		line_child_context is Line.ParseContext:
			... // ... self.lexeme.line_child_context
	Loadable is Self // Loadable{inherit: Node} ie. // Self = Node
		loaded is Boolean: false
		on load{ctx is Context}: ...
		Directory is Self
			directory is use.file.Directory
			on load{ctx is Context}:
				do subnode = directory.list |> as Node with ctx
		File is Self
			file is use.file.File
			line_child_context is Line.ParseContext: Line.ParseContext.FileRoot
			on load{ctx is Context}:
				line: file as many Line
				topline: Line.resolve_child line
				subnode: topline |> as Node with {ctx} // tokenize, astify, child_ctx, recurse/walk tree down
					ctx.parse: self.line_child_context
					ctx.main: use.parent.ctx
				do self.subnode = subnode

	--
	Context
		' scratch memory for when building/processing Node
			eg. hashmaps for memoization / to prevent cyclic infinity (see Dynamic Programming)
			eg. index of values to prevent need for full-search
				eg. to rename an identifier
					- ctx.identifier.(name-old).node |> .line |> .lexeme.token.start [> do .change name-new
						// TODO: detect computed names + type detect if name-old is a possible value, if so, warn
		// id could be some sort of hash, or just added index
		repository is many (id: Repository)
		directory is many (id: Directory)
		file is many (id: File)
		line is many (id: Line)
		node is many (id: Node)
		node.unloaded is many (id: Node.Loadable)
		connection is many (id: Connection)
		identifier is many (id: Identifier)
		identifier.unresolved is many (id: Identifier)
		issue.{syntax, lint, identifier, type, logic} is many (id: Issue)
		is Normalizable

SyntaxPackage
	- KeyValue
	- ListItem
	- CommaList
	- Range

Connection
	source is Node
	target is Node
	order is Float

VisualProxy
	node is Node
	...

ViewModel
	node is VisualProxy
	...

View
	model is ViewModel
	render: ...
	on interaction: ...


' note
	- no special treatment of eg. index files
		directory has index file anon created with all contents public
		keyword "use" (like rust) instead of import/include
		by convention, `use.local.token.Token; Token...` (dot syntax like python)
			in src/main.rim, if src/token.rim exists with `Token: ...` at top level
			`use src.main`
		// TODO: to be determined
		{Token}: use.dir.token // destructuring + "use" is just a node (imported by default, aka in the "prelude" (see rust)), with dynamic keyvalue management; "use.dir.some-file" = file node of "some-file.rim" in same dir as current file
		{main}: use.repo // "use.repo.some-file" = file node of repository/some-file.rim
			// no default use of src folder as common in other languages
			// convention: a main.rim at top-level in repo
			// 	it can use.dir.src.some-file.some-node if wanted



Normalizable
	as Normalized
		... fields |> or it.@type:
			- is many: |> normalize
			- else: | normalize
		normalize: or it.@type:
			- is (id: value is Normalizable): (.key): (.value as Normalized)
			- is ({id, ...} is Normalizable): {id}
		fields: use.parent.@fields